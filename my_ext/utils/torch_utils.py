#!/usr/bin/env python3
import contextlib
import functools
import math
import multiprocessing.managers
import re
import warnings
from collections import OrderedDict
from typing import Any, List, Tuple, TypeVar, Optional, Union
import logging

import numpy as np
import torch
import torch.distributed
import torch.optim.swa_utils
from torch import Tensor, nn

__all__ = ['to_tensor', 'tensor_to', 'convert_pth',
           'state_dict_strip_prefix_if_present', 'state_dict_add_prefix_if_not_present',
           'get_net', 'is_parallel', 'show_shape', 'set_printoptions', 'net_no_sync', 'get_GPU_memory',
           'disabled_train', 'split_run', 'sum_losses', 'get_interpolate_weight'
           ]


def to_tensor(x, dtype=None, device=None, **kwargs) -> Optional[Tensor]:
    if isinstance(x, torch.Tensor):
        y = x
    elif isinstance(x, np.ndarray):
        y = torch.from_numpy(x)
    # elif isinstance(x, (list, tuple)):
    #     x = np.ndarray(x)
    #     x = torch.from_numpy(x)
    elif x is None:
        return None
    else:
        y = torch.tensor(x, dtype=dtype, device=device)
    if dtype is not None:
        y = y.type(dtype)
    if device is not None:
        y = y.to(device=device, **kwargs)
    return y


def is_parallel(model):
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)


def get_net(model: nn.Module) -> nn.Module:
    if isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)):
        return model.module
    else:
        return model


def tensor_to(*args, device=None, non_blocking=True, **kwargs):
    results = []
    for x in args:
        if isinstance(x, torch.Tensor):
            results.append(x.to(device=device, non_blocking=non_blocking, **kwargs))
        elif isinstance(x, np.ndarray):
            results.append(torch.from_numpy(x).to(device=device, non_blocking=non_blocking, **kwargs))
        elif isinstance(x, tuple):
            n = len(x)
            x = tensor_to(*x, device=device, non_blocking=non_blocking, **kwargs)
            results.append(tuple(x) if n > 1 else (x,))
        elif isinstance(x, list):
            n = len(x)
            x = tensor_to(*x, device=device, non_blocking=non_blocking, **kwargs)
            results.append(list(x) if n > 1 else [x])
        elif isinstance(x, dict):
            results.append({k: tensor_to(v, device=device, non_blocking=non_blocking, **kwargs) for k, v in x.items()})
        elif isinstance(x, (int, float, bool)):
            results.append(x)
        elif hasattr(x, 'to'):
            results.append(getattr(x, 'to')(device=device, non_blocking=non_blocking, **kwargs))
        elif isinstance(x, (str,)):
            results.append(x)
        elif x is None:
            results.append(x)
        else:
            warnings.warn(f'unknown type: {type(x)}')
            results.append(x)

    return results[0] if len(args) == 1 else results


def _no_grad_trunc_normal_(tensor: Tensor, mean: float, std: float, a: float, b: float) -> Tensor:
    # Method based on
    # https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    # Modified from
    # https://github.com/pytorch/pytorch/blob/master/torch/nn/init.py
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [lower, upper], then translate
        # to [2lower-1, 2upper-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)  # noqa

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def show_shape(*inputs) -> str:
    string = []
    for x in inputs:
        if isinstance(x, Tensor):
            if x.dtype == torch.float32:
                s = f'F32Tensor{list(x.shape)}'
            elif x.dtype == torch.float64:
                s = f'F64Tensor{list(x.shape)}'
            elif x.dtype == torch.half:
                s = f'F16Tensor{list(x.shape)}'
            elif x.dtype == torch.int32:
                s = f'IntTensor{list(x.shape)}'
            elif x.dtype == torch.int64:
                s = f'LongTensor{list(x.shape)}'
            elif x.dtype == torch.bool:
                s = f'BoolTensor{list(x.shape)}'
            elif x.dtype == torch.uint8:
                s = f'Uint8Tensor{list(x.shape)}'
            else:
                s = f"{str(x.dtype).split('.')[1]}Tensor{list(x.shape)}"
        elif isinstance(x, np.ndarray):
            s = f'numpy({tuple(x.shape)}, {x.dtype})'
        elif isinstance(x, (list, multiprocessing.managers.ListProxy)):
            s = '[' + show_shape(*x) + ']'
        elif isinstance(x, tuple):
            s = '(' + show_shape(*x) + ')'
        elif isinstance(x, set):
            s = '{' + show_shape(*x) + '}'
        elif isinstance(x, dict):
            s = '{' + ', '.join(f"{show_shape(k)}: {show_shape(v)}" for k, v in x.items()) + '}'
        elif isinstance(x, str):
            s = f"'{x}'"
        else:
            s = str(x)
        string.append(s)
    return string[0] if len(inputs) == 1 else ', '.join(string)


def state_dict_strip_prefix_if_present(state_dict, prefix='module.') -> OrderedDict:
    if not all(key.startswith(prefix) for key in state_dict.keys()):
        return state_dict
    n = len(prefix)
    stripped_state_dict = OrderedDict()
    for key, value in state_dict.items():
        stripped_state_dict[key[n:]] = value
    return stripped_state_dict


def state_dict_add_prefix_if_not_present(state_dict, prefix) -> OrderedDict:
    if any(key.startswith(prefix) for key in state_dict.keys()):
        return state_dict
    stripped_state_dict = OrderedDict()
    for key, value in state_dict.items():
        stripped_state_dict[prefix + key] = value
    return stripped_state_dict


def net_no_sync(net: nn.Module, enable=True):
    """判断net是否使用DDP, 返回DDP的no_sync, 减少不必要的梯度同步"""
    if enable and isinstance(net, nn.parallel.DistributedDataParallel):
        return net.no_sync()
    else:
        return contextlib.nullcontext()


def convert_pth(pth, key=None, replace=None, bgr_to_rgb=False, first_layer_name=None, merge_dict=None):
    # type: (dict, Optional[str], Optional[dict], bool, Optional[str], Optional[List[str]]) -> dict
    # merge the state_dict of multi models
    if merge_dict:
        tmp = {}
        for k in merge_dict:
            v = pth[k]
            if isinstance(v, dict):
                for k2, v2 in v.items():
                    tmp[k + '.' + k2] = v2
            else:
                tmp[k] = v
        pth = tmp
    # get pretrained model in checkpoint
    not_a_checkpoint = not (isinstance(pth, dict) and all(isinstance(v, Tensor) for v in pth.values()))
    if not_a_checkpoint:
        if key is None or key not in pth:  # gauss correct key
            key = None
            for gauss_key in ['state_dict', 'model', 'network']:
                if gauss_key in pth:
                    key = gauss_key
        if key is not None:
            pth = pth[key]
            not_a_checkpoint = not (isinstance(pth, dict) and all(isinstance(v, Tensor) for v in pth.values()))
    if not_a_checkpoint:
        logging.error(f'The loaded pth is a checkpoint, please point out the key of state_dict in {pth.keys()}')
        return {}

    # deal first layer
    if bgr_to_rgb:
        if first_layer_name is None:
            first_layer_name = list(pth.keys())[0]
        v = pth[first_layer_name]
        assert v.ndim == 4 and v.size(1) == 3, f"{first_layer_name} is not first layer"
        pth[first_layer_name] = pth[first_layer_name][:, (2, 1, 0), :, :]
    # if Image.CHANNELS > 3:
    #     for name, value in pth.items():  # type: str, torch.Tensor
    #         if value.ndim == 4 and value.size(1) == 3:
    #             additional_shape = value.shape[0], Image.CHANNELS - 3, *value.shape[2:]
    #             pth[name] = torch.cat([value, value.new_zeros(additional_shape)], dim=1)

    pth = state_dict_strip_prefix_if_present(pth, 'module.')
    # 基于正则表达，重命名pth中的key
    if replace is not None:
        new_pth = OrderedDict()
        for k, v in pth.items():
            deal_time = 0
            for a, b in replace.items():
                match_result = re.match(a, k)
                if match_result:
                    k = b.format(*match_result.groups())
                    if k:  # skip this weight
                        new_pth[k] = v
                    deal_time += 1
                    break  # 多个匹配时，在前的先起作用
            if deal_time == 0:  # 不在替换列表中，保持原样
                new_pth[k] = v
            # assert deal_time == 1, f"Please check replace list, error for {k}"
    else:
        new_pth = pth
    return new_pth


def set_printoptions(precision=4, threshold=1000, edgeitems=3, linewidth=200, sci_mode=False, **kwargs):
    torch.set_printoptions(precision, threshold, edgeitems, linewidth, sci_mode=sci_mode)
    np.set_printoptions(precision, threshold, edgeitems, linewidth, suppress=not sci_mode)


def get_GPU_memory(reset=True, device=None, unit=1024 ** 3) -> Tuple[float, float]:
    """返回GPU利用情况 (最大使用，总量) 单位: GB"""
    total = torch.cuda.mem_get_info(device)[1] / unit
    max_used = torch.cuda.max_memory_allocated(device) / unit
    if reset:
        torch.cuda.reset_peak_memory_stats(device)
    return max_used, total


def disabled_train(self, mode=True):
    """Overwrite model.train with this function to make sure train/eval mode
    does not change anymore."""
    return self


T_fn = TypeVar('T_fn')


def split_run(fn: T_fn, split_size=-1, **keep_kwargs) -> T_fn:
    """将fn中args和kwargs中所有Tensor的第一维安装split_size进行分割，批次运行函数fn
    将所有fn的返回结果合并在一起, 结果中Tensor或np.ndarray按dim=0进行cat, 非tuple,list,dict则返回第一个结果的值
    e.g. [({'a': x}, None), ({'a': x}, None)] --> ({'a': x_cat}, None)
    """

    def cat_results(batch_results: List[Any]) -> Any:
        item = batch_results[0]
        if isinstance(item, tuple):
            return (cat_results([x[i] for x in batch_results]) for i in range(len(item)))
        elif isinstance(item, list):
            return [cat_results([x[i] for x in batch_results]) for i in range(len(item))]
        elif isinstance(item, dict):
            return {k: cat_results([x[k] for x in batch_results]) for k in item.keys()}
        elif isinstance(item, Tensor):
            return torch.cat(batch_results, dim=0)
        elif isinstance(item, np.ndarray):
            return np.concatenate(batch_results, axis=0)
        else:
            return item

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        if len(args) > 0:
            N = args[0].shape[0]
        elif len(kwargs) > 0:
            N = next(kwargs.values()).shape[0]  # noqa
        else:
            N = 0
        if 0 < split_size < N:
            results = []
            for i in range(0, N, split_size):
                part_i_args = [x[i:i + split_size] if isinstance(x, Tensor) else x for x in args]
                part_i_kwargs = {k: (v[i:i + split_size] if isinstance(v, Tensor) else v) for k, v in kwargs.items()}
                results.append(fn(*part_i_args, **part_i_kwargs, **keep_kwargs))
            return cat_results(results)
        else:
            return fn(*args, **keep_kwargs, **kwargs)

    return wrapper


def sum_losses(loss_dict: dict, weights: dict = None) -> Tensor:
    if weights is None:
        weights = {}
    total = 0
    for k, v in loss_dict.items():
        total = v * weights.get(k, 1.) + total
    return total


def get_interpolate_weight(xs: Tensor, x: Union[Tensor, float]):
    """ find nearest two value for x in array xs, where xs is sorted
    Args:
        xs: sorted array
        x: query value
    Returns:
        (w, idx1, idx2), therefore, x = torch.lerp(xs[idx1], xs[idx2], w)
    """
    idx = torch.searchsorted(xs, x).item()
    N = len(xs)
    if idx == len(xs):
        idx1, idx2 = N - 2, N - 1
    elif idx == 0:
        idx1, idx2 = idx, idx + 1
    else:
        idx1, idx2 = idx - 1, idx
    x1, x2 = xs[idx1], xs[idx2]
    w = (x - x1) / (x2 - x1)
    return w, idx1, idx2
